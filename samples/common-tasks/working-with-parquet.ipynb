{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Block Model API + Parquet Files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "To get started with this notebook it's recommended to create a new Python virtual environment and to install `requirements.txt` in the `blockmodels/python` folder, eg.\n",
    "\n",
    "`> pip install -r requirements.txt`\n",
    "\n",
    "Apache Arrow is a cross-language development platform for in-memory analytics, and PyArrow is the Python API for Apache Arrow.\n",
    "PyArrow is used extensively in this notebook and you should become familiar with its capabilities.\n",
    "\n",
    "### Data types and naming restrictions\n",
    "\n",
    "Evo supports block models with the following Parquet data types:\n",
    "\n",
    "* boolean\n",
    "* int8/16/32/64\n",
    "* float16/32/64\n",
    "* utf8\n",
    "* date32\n",
    "* timestamp\n",
    "    * unit = microseconds (us)\n",
    "    * tz = UTC\n",
    "\n",
    "**Note:** Apache Arrow natively uses lower case data types, e.g. float64, while the Block Model API uses enumerated versions of these data types with capitalisation. e.g. Float64.\n",
    "\n",
    "Below are the reserved system attributes (column names):\n",
    "\n",
    "* `i`, `j`, `k` - parent block indexes (uint32)\n",
    "* `sidx` - sub-block indexes (uint32)\n",
    "* `dx`, `dy`, `dz` - block sizes (float64 - is not stored but is calculated on the fly)\n",
    "* `x`, `y`, `z` - block centroids (float64 - is not stored but is calculated on the fly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Convert a block model in CSV format to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert a regular block model CSV file to an Evo-compatible Parquet file.\n",
    "# The input CSV contains block model centroids and several data columns with non-UUID column names.\n",
    "\n",
    "import pprint\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Local copy of the Parquet file being uploaded can have any name since it will be renamed by Evo when saved in the Azure blob store.\n",
    "# Reserved system column names are: x, y, z, dx, dy, dz, i, j, k, sidx.\n",
    "# Non-system columns must be given valid UUIDs as their columns names and any API requests made must match the UUIDs generated in the CSV file.\n",
    "\n",
    "# Part 1 - Convert CSV to Parquet\n",
    "\n",
    "input_csv = \"data/example1/regular-bm-named-columns.csv\"\n",
    "input_csv_path = Path(input_csv)\n",
    "output_parquet = input_csv_path.parent / f\"{input_csv_path.stem}.parquet\"\n",
    "\n",
    "csv_columns = {\n",
    "    \"x\": pa.float64(),\n",
    "    \"y\": pa.float64(),\n",
    "    \"z\": pa.float64(),  # The service expects centroid columns in a Parquet file to be lowercase 'x', 'y', 'z' and must all be float64 Parquet data type\n",
    "    \"Lith\": pa.utf8(),  # List the column names and required data types\n",
    "    \"LMS1_Pct\": pa.float32(),  # Parquet files do not support white space in column names, be sure to remove any spaces in CSV column names!\n",
    "    \"MV_Pct\": pa.float32(),\n",
    "    \"Blocks\": pa.utf8(),\n",
    "}\n",
    "\n",
    "options = pv.ConvertOptions(column_types=csv_columns)\n",
    "table = pv.read_csv(str(input_csv_path), convert_options=options)\n",
    "\n",
    "# Part 2 - Rename original CSV columns names to compatible UUIDs\n",
    "\n",
    "[csv_columns.pop(name, None) for name in [\"x\", \"y\", \"z\"]]  # Remove system columns names from CSV columns\n",
    "\n",
    "# Get list of CSV names\n",
    "csv_names = list(csv_columns.keys())\n",
    "\n",
    "# Generate list of UUIDs\n",
    "uuid_names = [str(uuid.uuid4()) for _ in csv_names]\n",
    "\n",
    "names_new = dict(zip(uuid_names, csv_names))  # Create CSV name : uuid dictionary\n",
    "merged_new_names = {\n",
    "    \"x\": \"x\",\n",
    "    \"y\": \"y\",\n",
    "    \"z\": \"z\",\n",
    "}  # Add x, y, z default values back to dictionary\n",
    "merged_new_names.update(names_new)\n",
    "\n",
    "# The table.rename_columns function might not be appropriate for large files.\n",
    "# If this is the case for you, see code cell #5 for an example of how to edit columns names after Parquet creation.\n",
    "table_rename = table.rename_columns(merged_new_names)  # Replace CSV column names with generated uuid column names\n",
    "\n",
    "# Evo expects Parquet files to be created with the following parameters:\n",
    "# - compression='zstd'\n",
    "# - write_statistics=True\n",
    "# - version='2.6' (it's recommended to use the latest Parquet version currently available)\n",
    "pq.write_table(\n",
    "    table_rename,\n",
    "    output_parquet,\n",
    "    compression=\"zstd\",\n",
    "    row_group_size=100_000,\n",
    "    version=\"2.6\",\n",
    "    data_page_version=\"1.0\",\n",
    "    write_statistics=True,\n",
    ")\n",
    "\n",
    "print(\"Column mapping to use with the Block Model API:\")\n",
    "pprint.pprint(merged_new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Convert a regular block model CSV file to an Evo-compatible Parquet file.\n",
    "# The input CSV contains block model centroids and several data columns that already have UUIDs as the column headers.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Local copy of the Parquet file being uploaded can have any name since it will be renamed by Evo when saved in the Azure blob store.\n",
    "# Reserved system column names are: x, y, z, dx, dy, dz, i, j, k, sidx.\n",
    "# Non-system columns must be given valid UUIDs as their columns names and any API requests made to Evo must match the UUIDs generated in the CSV file.\n",
    "\n",
    "# Tip: Use the following Excel formula to generate random UUIDs:\n",
    "# =LOWER(CONCATENATE(DEC2HEX(RANDBETWEEN(0,4294967295),8),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,4294967295),8),DEC2HEX(RANDBETWEEN(0,65535),4)))\n",
    "\n",
    "input_csv = \"data/example2/regular-bm-uuid-columns.csv\"\n",
    "input_csv_path = Path(input_csv)  # UUIDs were generated with the Excel code above\n",
    "output_parquet = input_csv_path.parent / f\"{input_csv_path.stem}.parquet\"\n",
    "\n",
    "csv_columns = {\n",
    "    \"x\": pa.float64(),\n",
    "    \"y\": pa.float64(),\n",
    "    \"z\": pa.float64(),  # Evo expects centroid columns in a Parquet file to be lowercase 'x', 'y', 'z' and to all be float64 pyarrow data type\n",
    "    \"68002a18-5f56-57d2-e6c3-5dd87d3b7ad9\": pa.utf8(),  # Lith\n",
    "    \"688d284c-d2be-5219-b38a-0712a8fa6058\": pa.float32(),  # LMS1_Pct\n",
    "    \"a88f0e41-2dda-6417-3fce-69f19eeabe29\": pa.float32(),  # MV_Pct\n",
    "    \"a1757e57-3fd4-0e65-87fd-bfa98abc3caa\": pa.utf8(),  # Blocks\n",
    "}\n",
    "\n",
    "options = pv.ConvertOptions(column_types=csv_columns)\n",
    "table = pv.read_csv(str(input_csv_path), convert_options=options)\n",
    "\n",
    "# Evo expects Parquet files to be created with the following parameters:\n",
    "# - compression='zstd'\n",
    "# - write_statistics=True\n",
    "# - version='2.6' (it's recommended to use the latest Parquet version currently available)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    output_parquet,\n",
    "    compression=\"zstd\",\n",
    "    row_group_size=100_000,\n",
    "    version=\"2.6\",\n",
    "    data_page_version=\"1.0\",\n",
    "    write_statistics=True,\n",
    ")\n",
    "\n",
    "print(\"Data types used were:\")\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Convert a variable-octree block model CSV file to an Evo-compatible Parquet file.\n",
    "# The input CSV containing block indices and several data columns with non-UUID column names.\n",
    "\n",
    "import pprint\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Local client copy of the Parquet file being uploaded can have any name since it will be renamed by Evo when saved in the Azure blob store.\n",
    "# Reserved system column names are: x, y, z, dx, dy, dz, i, j, k, sidx.\n",
    "# Non-system columns must be given valid UUIDs as their columns names and any API requests made to Evo must match the UUIDs generated in the CSV file.\n",
    "\n",
    "# Part 1 - Convert CSV to Parquet\n",
    "\n",
    "input_csv = \"data/example3/variable-octree-bm-named-columns.csv\"\n",
    "input_csv_path = Path(input_csv)\n",
    "output_parquet = input_csv_path.parent / f\"{input_csv_path.stem}.parquet\"\n",
    "\n",
    "csv_columns = {\n",
    "    \"i\": pa.uint32(),\n",
    "    \"j\": pa.uint32(),\n",
    "    \"k\": pa.uint32(),\n",
    "    \"sidx\": pa.uint32(),  # Evo expects block model indices in a Parquet file to be lowercase 'i', 'j', 'k', 'sidx' and to all be uint32 pyarrow data type\n",
    "    \"a0\": pa.float32(),  # List the column names and required data types\n",
    "    \"a1\": pa.utf8(),\n",
    "    \"a2\": pa.bool_(),\n",
    "}\n",
    "\n",
    "options = pv.ConvertOptions(column_types=csv_columns)\n",
    "table = pv.read_csv(str(input_csv_path), convert_options=options)\n",
    "\n",
    "# Part 2 - Rename original CSV columns names to compatible UUIDs\n",
    "\n",
    "[csv_columns.pop(name, None) for name in [\"i\", \"j\", \"k\", \"sidx\"]]  # Remove system columns names from CSV columns\n",
    "\n",
    "# Get list of CSV names\n",
    "csv_names = list(csv_columns.keys())\n",
    "\n",
    "# Generate list of UUIDs\n",
    "uuid_names = [str(uuid.uuid4()) for _ in csv_names]\n",
    "\n",
    "names_new = dict(zip(uuid_names, csv_names))  # Create CSV name : uuid dictionary\n",
    "merged_new_names = {\n",
    "    \"i\": \"i\",\n",
    "    \"j\": \"j\",\n",
    "    \"k\": \"k\",\n",
    "    \"sidx\": \"sidx\",\n",
    "}  # Add x, y, z default values back to dictionary\n",
    "merged_new_names.update(names_new)\n",
    "\n",
    "# The table.rename_columns function might not be appropriate for large files.\n",
    "# If this is the case for you, see code cell #5 for an example of how to edit columns names after Parquet creation.\n",
    "table_rename = table.rename_columns(merged_new_names)  # Replace csv column names with generated uuid column names\n",
    "\n",
    "# Evo expects Parquet files to be created with the following parameters:\n",
    "# - compression='zstd'\n",
    "# - write_statistics=True\n",
    "# - version='2.6' (it's recommended to use the latest Parquet version currently available)\n",
    "pq.write_table(\n",
    "    table_rename,\n",
    "    output_parquet,\n",
    "    compression=\"zstd\",\n",
    "    row_group_size=100_000,\n",
    "    version=\"2.6\",\n",
    "    data_page_version=\"1.0\",\n",
    "    write_statistics=True,\n",
    ")\n",
    "\n",
    "print(\"Column mapping to use with the Block Model API:\")\n",
    "pprint.pprint(merged_new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert a variable-octree block model CSV file to an Evo-compatible Parquet file.\n",
    "# The input CSV containing block model indices and several data columns that have UUIDs as the column headers.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Local client copy of the Parquet file being uploaded can have any name since it will be renamed by Evo when saved in the Azure blob store.\n",
    "# Reserved system column names are: x, y, z, dx, dy, dz, i, j, k, sidx.\n",
    "# Non-system columns must be given valid UUIDs as their columns names and any API requests must match the UUIDs generated in the CSV file.\n",
    "\n",
    "# Tip: Use the following Excel formula to generate random UUIDs:\n",
    "# =LOWER(CONCATENATE(DEC2HEX(RANDBETWEEN(0,4294967295),8),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,65535),4),\"-\",DEC2HEX(RANDBETWEEN(0,4294967295),8),DEC2HEX(RANDBETWEEN(0,65535),4)))\n",
    "\n",
    "input_csv = \"data/example4/variable-octree-bm-uuid-columns.csv\"  # edit to match CSV path\n",
    "input_csv_path = Path(input_csv)  # UUIDs were generated with the Excel code above\n",
    "output_parquet = input_csv_path.parent / f\"{input_csv_path.stem}.parquet\"\n",
    "\n",
    "csv_columns = {\n",
    "    \"x\": pa.float64(),\n",
    "    \"y\": pa.float64(),\n",
    "    \"z\": pa.float64(),  # Evo expects centroid columns in a Parquet file to be lowercase 'x', 'y', 'z' and to all be float64 Parquet data type\n",
    "    \"da0f878c-c524-622b-50a5-a1f39b6c2075\": pa.float32(),  # a0\n",
    "    \"12b66d3d-838e-2ffe-fd52-9d5144e33e5e\": pa.utf8(),  # a1\n",
    "    \"e3f3cfcf-5bc4-f7c9-5364-72151d2f9d70\": pa.bool_(),  # a2\n",
    "}\n",
    "\n",
    "options = pv.ConvertOptions(column_types=csv_columns)\n",
    "table = pv.read_csv(str(input_csv_path), convert_options=options)\n",
    "\n",
    "# Evo expects Parquet files to be created with the following parameters:\n",
    "# - compression='zstd'\n",
    "# - write_statistics=True\n",
    "# - version='2.6' (it's recommended to use the latest Parquet version currently available)\n",
    "pq.write_table(\n",
    "    table,\n",
    "    output_parquet,\n",
    "    compression=\"zstd\",\n",
    "    row_group_size=100_000,\n",
    "    version=\"2.6\",\n",
    "    data_page_version=\"1.0\",\n",
    "    write_statistics=True,\n",
    ")\n",
    "\n",
    "print(\"Data types used were:\")\n",
    "print(table.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Parquet File Handling\n",
    "\n",
    "The following code cells include functions for common Parquet file handling tasks:\n",
    "- Rename a column in a Parquet file.\n",
    "- Recast a column data type in a Parquet file.\n",
    "- Drop columns from a Parquet file.\n",
    "- Convert a Parquet file to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Rename columns in a Parquet file after it has been created.\n",
    "# Previous code cells in this notebook renamed columns during the Parquet creation process.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def rename(pq_file, name_update_dict):\n",
    "    pq_file = Path(pq_file)\n",
    "    pq_out_path = pq_file.parent / f\"{pq_file.stem}-columns-renamed.parquet\"\n",
    "    parquet_file = pq.ParquetFile(pq_file)\n",
    "    row_groups = parquet_file.num_row_groups\n",
    "    schema = parquet_file.schema_arrow\n",
    "\n",
    "    # Check update all keys exist in the schema\n",
    "    if not any(item in schema.names for item in list(name_update_dict.keys())):\n",
    "        print(\"Not all of your update fields were found in the Parquet file schema, exiting now\")\n",
    "        exit()\n",
    "\n",
    "    # Rename columns in schema\n",
    "    new_fields = []\n",
    "    for each in schema:\n",
    "        if each.name in list(name_update_dict.keys()):\n",
    "            new_fields.append(pa.field(name=name_update_dict[each.name], type=each.type))\n",
    "        else:\n",
    "            new_fields.append(pa.field(name=each.name, type=each.type))\n",
    "    new_schema = pa.schema(new_fields)\n",
    "\n",
    "    # For each row group get the table, rename the columns and drop the others\n",
    "    for grp in range(0, row_groups):\n",
    "        table_orig = parquet_file.read_row_group(grp)\n",
    "        table_rename = table_orig.rename_columns(new_schema.names)\n",
    "\n",
    "        if grp == 0:\n",
    "            pqwriter = pq.ParquetWriter(\n",
    "                str(pq_out_path), compression=\"zstd\", schema=new_schema, version=\"2.6\"\n",
    "            )  # Do this on first group to create the schema\n",
    "        try:\n",
    "            pqwriter.write_table(table_rename)\n",
    "        except Exception:\n",
    "            print(\"Error occurred in row_group \" + str(grp))\n",
    "            exit()\n",
    "\n",
    "        # Close the Parquet writer\n",
    "    if pqwriter:\n",
    "        pqwriter.close()\n",
    "    else:\n",
    "        print(\"pqwriter had already closed\")\n",
    "\n",
    "\n",
    "filepath = \"data/example5/bm.parquet\"\n",
    "column_name_update = {\n",
    "    \"68002a18-5f56-57d2-e6c3-5dd87d3b7ad9\": \"renamed_1\",\n",
    "    \"a1757e57-3fd4-0e65-87fd-bfa98abc3caa\": \"renamed_2\",\n",
    "}  # {old: new}\n",
    "\n",
    "rename(filepath, column_name_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Recast a column data type in a Parquet file to a different pyarrow data type.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def recast(pq_file, update_columns, new_type):\n",
    "    pq_file = Path(pq_file)\n",
    "    pq_out_path = pq_file.parent / f\"{pq_file.stem}-new-data-type.parquet\"\n",
    "    parquet_file = pq.ParquetFile(pq_file)\n",
    "    row_groups = parquet_file.num_row_groups\n",
    "    schema = parquet_file.schema_arrow\n",
    "\n",
    "    # Check all update_columns exist in the schema\n",
    "    if not any(item in schema.names for item in update_columns):\n",
    "        print(\"Not all of your update fields were found in the Parquet file schema, exiting now\")\n",
    "        exit()\n",
    "\n",
    "    # Build new schema with new types\n",
    "    new_fields = []\n",
    "    for each in schema:\n",
    "        if each.name in update_columns:\n",
    "            new_fields.append(pa.field(name=each.name, type=new_type))\n",
    "        else:\n",
    "            new_fields.append(pa.field(name=each.name, type=each.type))\n",
    "\n",
    "    new_schema = pa.schema(new_fields)\n",
    "\n",
    "    # For each row group get the table, then get the column, recast the column if required\n",
    "    for grp in range(0, row_groups):\n",
    "        table_orig = parquet_file.read_row_group(grp)\n",
    "        chunked_arrays = []\n",
    "\n",
    "        # Go through each table column, get it's name, then set it's type from the schema\n",
    "        for idx, col in enumerate(table_orig):\n",
    "            name = table_orig.column_names[idx]\n",
    "            chunked_arrays.append(table_orig.column(name).cast(new_schema.field(name).type, safe=True))\n",
    "        # Build table from list of chunked arrays\n",
    "        new_table = pa.Table.from_arrays(chunked_arrays, schema=new_schema)\n",
    "\n",
    "        # Write the new table to file\n",
    "        if grp == 0:\n",
    "            pqwriter = pq.ParquetWriter(\n",
    "                str(pq_out_path), compression=\"zstd\", schema=new_schema, version=\"2.6\"\n",
    "            )  # Do this on first group to create the schema\n",
    "        try:\n",
    "            pqwriter.write_table(new_table)\n",
    "        except Exception:\n",
    "            print(\"Error occurred in row_group \" + str(grp))\n",
    "            exit()\n",
    "\n",
    "        # Close the Parquet writer\n",
    "    if pqwriter:\n",
    "        pqwriter.close()\n",
    "    else:\n",
    "        print(\"pqwriter had already closed\")\n",
    "\n",
    "\n",
    "file_path = \"data/example6/bm.parquet\"\n",
    "update_columns = [\"a88f0e41-2dda-6417-3fce-69f19eeabe29\"]  # UUID header for MV_Pct column\n",
    "new_type = pa.float64()\n",
    "\n",
    "recast(file_path, update_columns, new_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Take a list of field names that you want to keep and drops all others from a Parquet file.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def reduce_columns(pq_file, fields_to_keep):\n",
    "    # Read Parquet file\n",
    "    parquet_file = pq.ParquetFile(str(pq_file))\n",
    "    all_cols = parquet_file.schema.names\n",
    "    fields_to_drop = [x for x in all_cols if x not in fields_to_keep]\n",
    "    schema = parquet_file.schema_arrow\n",
    "\n",
    "    pq_out_path = pq_file.parent / f\"{pq_file.stem}-reduced-columns.parquet\"\n",
    "\n",
    "    # Prepare reduce schema\n",
    "    for field in fields_to_drop:\n",
    "        idx = schema.get_field_index(field)\n",
    "        schema = schema.remove(idx)\n",
    "\n",
    "    row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    for i in range(row_groups):\n",
    "        table_in = parquet_file.read_row_group(i)\n",
    "        table_in = table_in.drop(fields_to_drop)\n",
    "\n",
    "        if i == 0:\n",
    "            pqwriter = pq.ParquetWriter(\n",
    "                str(pq_out_path), schema, compression=\"zstd\", version=\"2.6\"\n",
    "            )  # Do this on first group to create the schema\n",
    "        try:\n",
    "            pqwriter.write_table(table_in)\n",
    "        except Exception:\n",
    "            print(\"Error occurred in row_group \" + str(i))\n",
    "            exit()\n",
    "\n",
    "    # Close the Parquet writer\n",
    "    if pqwriter:\n",
    "        pqwriter.close()\n",
    "    else:\n",
    "        print(\"pqwriter had already closed\")\n",
    "\n",
    "\n",
    "pq_file = Path(\"data/example7/bm.parquet\")\n",
    "fields_to_keep = [\n",
    "    \"a88f0e41-2dda-6417-3fce-69f19eeabe29\",\n",
    "    \"68002a18-5f56-57d2-e6c3-5dd87d3b7ad9\",\n",
    "]  # Columns not in this list will be removed from the Parquet file\n",
    "\n",
    "reduce_columns(pq_file, fields_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Convert a Parquet file to CSV.\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def convert_to_csv(pq_file):\n",
    "    pq_file = Path(pq_file)\n",
    "    csv_out_path = pq_file.parent / f\"{pq_file.stem}.csv\"\n",
    "    parquet_file = pq.ParquetFile(pq_file)\n",
    "    row_groups = parquet_file.num_row_groups\n",
    "\n",
    "    for grp in range(0, row_groups):\n",
    "        table = parquet_file.read_row_group(grp)\n",
    "        df = table.to_pandas()\n",
    "        if grp == 0:\n",
    "            df.to_csv(\n",
    "                csv_out_path,\n",
    "                sep=\",\",\n",
    "                header=True,\n",
    "                index=False,\n",
    "                mode=\"w\",\n",
    "                lineterminator=\"\\r\\n\",\n",
    "            )\n",
    "        else:\n",
    "            df.to_csv(\n",
    "                csv_out_path,\n",
    "                sep=\",\",\n",
    "                header=False,\n",
    "                index=False,\n",
    "                mode=\"a\",\n",
    "                lineterminator=\"\\r\\n\",\n",
    "            )\n",
    "\n",
    "\n",
    "file_path = \"data/example8/bm.parquet\"  # Parquet file to be converted to CSV\n",
    "\n",
    "convert_to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
