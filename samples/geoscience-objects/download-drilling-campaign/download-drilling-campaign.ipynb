{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Download a Drilling Campaign object and save it in CSV format\n",
    "\n",
    "This example shows how to download a drilling-campaign object from an Evo workspace and how to construct CSV files from the data.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You must have a Seequent account with the Evo entitlement to use this notebook.\n",
    "\n",
    "The following parameters must be provided:\n",
    "\n",
    "- The client ID of your Evo application.\n",
    "- The callback/redirect URL of your Evo application.\n",
    "\n",
    "To obtain these app credentials, refer to the [Apps and tokens guide](https://developer.seequent.com/docs/guides/getting-started/apps-and-tokens) in the Seequent Developer Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from evo.notebooks import FeedbackWidget, ServiceManagerWidget\n",
    "from evo.objects import ObjectAPIClient\n",
    "\n",
    "cache_location = \"data\"\n",
    "\n",
    "# Evo app credentials\n",
    "client_id = \"<your-client-id>\"  # Replace with your client ID\n",
    "redirect_url = \"<your-redirect-url>\"  # Replace with your redirect URL\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    discovery_url=\"https://discover.api.seequent.com\",\n",
    "    redirect_url=redirect_url,\n",
    "    client_id=client_id,\n",
    "    cache_location=cache_location,\n",
    ").login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Use the Evo Python SDK to create an object client and a data client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object client will manage your auth token and Geoscience Object API requests.\n",
    "object_client = ObjectAPIClient(manager.get_environment(), manager.get_connector())\n",
    "\n",
    "# The data client will manage saving your data as Parquet and publishing your data to Evo storage.\n",
    "data_client = object_client.get_data_client(manager.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### List all objects in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "all_objects = await object_client.list_all_objects()\n",
    "\n",
    "table = PrettyTable([\"Name\", \"Object ID\"])\n",
    "for index, obj in enumerate(all_objects):\n",
    "    if \"drilling-campaign\" in obj.schema_id.sub_classification:\n",
    "        table.add_row([obj.name.ljust(40), str(obj.id).ljust(40)])\n",
    "\n",
    "if len(table.rows) == 0:\n",
    "    print(\"No drilling campaigns found.\")\n",
    "else:\n",
    "    print(\"Drilling campaigns found:\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Enter the `Object ID` value for the chosen drilling-campaign object in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = \"c5d1c97c-deca-4241-9f42-0b388efd0bbc\"\n",
    "\n",
    "\n",
    "all_versions = await object_client.list_versions_by_id(object_id)\n",
    "n_versions = len(all_versions)\n",
    "n_digits = len(str(n_versions))\n",
    "\n",
    "table = PrettyTable([\"Version #\", \"Version ID\", \"Author\", \"Created\"])\n",
    "table.add_row(\n",
    "    [\n",
    "        f\"Version {n_versions:>{n_digits}} (latest)\",\n",
    "        str(all_versions[0].version_id).rjust(20),\n",
    "        str(all_versions[0].created_by.name).ljust(20),\n",
    "        all_versions[0].created_at.strftime(\"%Y-%m-%d %H:%M\").ljust(15),\n",
    "    ]\n",
    ")\n",
    "all_version_ids = []\n",
    "for index, obj in enumerate(all_versions[1:]):\n",
    "    # obj: ObjectVersion\n",
    "    table.add_row(\n",
    "        [\n",
    "            f\"Version {n_versions - (index + 1):>{n_digits}}\",\n",
    "            str(obj.version_id).rjust(20),\n",
    "            str(obj.created_by.name).ljust(20),\n",
    "            obj.created_at.strftime(\"%Y-%m-%d %H:%M\").ljust(15),\n",
    "        ]\n",
    "    )\n",
    "    all_version_ids.append(obj.version_id)\n",
    "\n",
    "\n",
    "print(\"Versions found:\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Download the Parquet files and assemble the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id = \"1761266774698612662\"\n",
    "\n",
    "assert version_id == \"\" or version_id in all_version_ids, (\n",
    "    f\"Version ID {version_id} not found. Please select a valid version ID from the table above.\"\n",
    ")\n",
    "\n",
    "output_filename = \"planned_drillholes.xlsx\"\n",
    "overwrite_existing = True\n",
    "\n",
    "if not overwrite_existing and Path(output_filename).exists():\n",
    "    raise FileExistsError(f\"{output_filename} already exists. Set 'overwrite_existing' to True to overwrite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_id = version_id if version_id else None\n",
    "downloaded_object = await object_client.download_object_by_id(object_id=object_id)\n",
    "\n",
    "metadata = downloaded_object.metadata\n",
    "downloaded_dict = downloaded_object.as_dict()\n",
    "\n",
    "\n",
    "def download_table(table_info, fb=None):\n",
    "    if fb is None:\n",
    "        fb = FeedbackWidget(\"Downloading unknown table\")\n",
    "    return data_client.download_table(\n",
    "        object_id=metadata.id, version_id=metadata.version_id, table_info=table_info, fb=fb\n",
    "    )\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Use the data client to download the coordinate data.\n",
    "hole_indices = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"values\"],\n",
    "        fb=FeedbackWidget(f\"Downloading hole indices data as '{downloaded_dict['hole_id']['values']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "index_to_name_map = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"table\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading hole index to name map data as '{downloaded_dict['hole_id']['table']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "names = pd.DataFrame({\"key\": hole_indices[\"data\"]}).merge(index_to_name_map, on=\"key\", how=\"left\")[\"value\"]\n",
    "\n",
    "collar_locations = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"coordinates\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar locations data as '{downloaded_dict['planned']['collar']['coordinates']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "hole_lengths = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"distances\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar hole distances table as '{downloaded_dict['planned']['collar']['distances']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "chunk_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"holes\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar chunks data as '{downloaded_dict['planned']['collar']['holes']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "attributes = []\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "for attribute in downloaded_dict[\"planned\"][\"collar\"][\"attributes\"]:\n",
    "    attribute_name = attribute[\"name\"]\n",
    "    attribute_type = attribute[\"attribute_type\"]\n",
    "\n",
    "    # Download the attribute data. Every attribute has a 'values' data file.\n",
    "    values_data = (\n",
    "        await data_client.download_table(\n",
    "            object_id=metadata.id,\n",
    "            version_id=metadata.version_id,\n",
    "            table_info=attribute[\"values\"],\n",
    "            fb=FeedbackWidget(\n",
    "                f\"Downloading attribute '{attribute_name}' values data as '{attribute['values']['data']}'\"\n",
    "            ),\n",
    "        )\n",
    "    ).to_pandas()\n",
    "    attributes.append(values_data)\n",
    "\n",
    "df = pd.concat([names, collar_locations, hole_lengths, chunk_data, *attributes], axis=1)\n",
    "\n",
    "path_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"path\"],\n",
    "        fb=FeedbackWidget(f\"Downloading collar path data as '{downloaded_dict['planned']['path']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "processed_path_data = pd.concat(\n",
    "    [\n",
    "        path_data.iloc[start : start + length].reset_index(drop=True)\n",
    "        for start, length in zip(chunk_data[\"offset\"], chunk_data[\"count\"])\n",
    "    ],\n",
    "    axis=0,\n",
    ").reset_index(drop=True)\n",
    "hole_name = []\n",
    "\n",
    "for hole_id, count in zip(chunk_data[\"hole_index\"], chunk_data[\"count\"]):\n",
    "    hole_name.extend([index_to_name_map[index_to_name_map[\"key\"] == hole_id][\"value\"].values[0]] * count)\n",
    "processed_path_data[\"hole_name\"] = hole_name\n",
    "processed_path_data = processed_path_data[\n",
    "    [\"hole_name\"] + [col for col in processed_path_data.columns if col != \"hole_name\"]\n",
    "]\n",
    "\n",
    "attribute_tables = {}\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "if \"collections\" in downloaded_dict[\"planned\"]:\n",
    "    for attribute_table in downloaded_dict[\"planned\"][\"collections\"]:\n",
    "        collection_type = attribute_table[\"collection_type\"]\n",
    "        collection_name = f\"planned_{collection_type}_({attribute_table['name']})\"\n",
    "\n",
    "        if collection_type == \"interval\":\n",
    "            distance_container = attribute_table[\"from_to\"][\"intervals\"][\"start_and_end\"]\n",
    "            attribute_container = attribute_table[\"from_to\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        elif collection_type == \"distance\":\n",
    "            distance_container = attribute_table[\"intervals\"][\"start_and_end\"]\n",
    "            attribute_container = attribute_table[\"distance\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        attribute_chunk_data = (\n",
    "            await data_client.download_table(\n",
    "                object_id=metadata.id,\n",
    "                version_id=metadata.version_id,\n",
    "                table_info=attribute_table[\"holes\"],\n",
    "                fb=FeedbackWidget(f\"Downloading attribute chunk data as '{attribute_table['holes']['data']}'\"),\n",
    "            )\n",
    "        ).to_pandas()\n",
    "\n",
    "        columns = [distance_data]\n",
    "        for column in attribute_container:\n",
    "            attribute_name = column[\"name\"]\n",
    "            attribute_type = column[\"attribute_type\"]\n",
    "\n",
    "            column_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=column[\"values\"],\n",
    "                    fb=FeedbackWidget(\n",
    "                        f\"Downloading attribute '{attribute_name}' column data as '{column['values']['data']}'\"\n",
    "                    ),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "            column_data.columns = [\"data\"]\n",
    "\n",
    "            # If the attribute is a category, download the 'table' data as well.\n",
    "            if attribute_type == \"category\":\n",
    "                lookup_table = (\n",
    "                    await data_client.download_table(\n",
    "                        object_id=metadata.id,\n",
    "                        version_id=metadata.version_id,\n",
    "                        table_info=column[\"table\"],\n",
    "                        fb=FeedbackWidget(\n",
    "                            f\"Downloading attribute '{attribute_name}' lookup table data as '{column['table']['data']}'\"\n",
    "                        ),\n",
    "                    )\n",
    "                ).to_pandas()\n",
    "\n",
    "                # Merge the values data with the table data.\n",
    "                merged_data = pd.merge(column_data, lookup_table, left_on=\"data\", right_on=\"key\", how=\"left\")\n",
    "                # Drop the 'data' and 'key' columns from the merged data.\n",
    "                merged_data.drop(columns=[\"data\", \"key\"], inplace=True)\n",
    "                # Rename the 'value' column to the attribute name.\n",
    "                merged_data.rename(columns={\"value\": attribute_name}, inplace=True)\n",
    "                # Concatenate the merged data with the coordinates data.\n",
    "                columns.append(merged_data)\n",
    "\n",
    "            elif attribute_type == \"scalar\":\n",
    "                # Rename the 'data' column to the attribute name.\n",
    "                column_data.rename(columns={\"data\": attribute_name}, inplace=True)\n",
    "                # Concatenate the data with the coordinates data.\n",
    "                columns.append(column_data)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown attribute type: {attribute_type}\")\n",
    "\n",
    "        attribute_tables[collection_name] = pd.concat(columns, axis=1)\n",
    "        attribute_tables[f\"{collection_name} (Cnk)\"] = attribute_chunk_data\n",
    "\n",
    "# Save the dataframe as a CSV file.\n",
    "with pd.ExcelWriter(output_filename) as writer:\n",
    "    df.to_excel(writer, sheet_name=\"Collars\", index=False)\n",
    "    path_data.to_excel(writer, sheet_name=\"Raw Paths\", index=False)\n",
    "    processed_path_data.to_excel(writer, sheet_name=\"Paths\", index=False)\n",
    "    for name, table in attribute_tables.items():\n",
    "        table.to_excel(writer, sheet_name=name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of files in the cache_location directory (including subdirectories)\n",
    "downloaded_files = Path(cache_location).glob(\"**/*\")\n",
    "\n",
    "# Iterate through each file and rename to add '.parquet' extension\n",
    "for file_path in downloaded_files:\n",
    "    if (\n",
    "        file_path.is_file()\n",
    "        and not file_path.name.endswith(\".parquet\")\n",
    "        and not file_path.name.startswith(\".\")\n",
    "        and not file_path.suffix\n",
    "    ):  # Only rename files with no extension\n",
    "        new_path = file_path.with_suffix(file_path.suffix + \".parquet\")\n",
    "        file_path.rename(new_path)\n",
    "        print(f\"Renamed: {file_path.name} -> {new_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
