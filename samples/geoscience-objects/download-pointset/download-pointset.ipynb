{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a Pointset object and save it in CSV format\n",
    "\n",
    "This example shows how to download a pointset object from an Evo workspace and how to construct a CSV file from the pointset components.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You must have a Seequent account with the Evo entitlement to use this notebook.\n",
    "\n",
    "The following parameters must be provided:\n",
    "\n",
    "- The client ID of your Evo application.\n",
    "- The callback/redirect URL of your Evo application.\n",
    "\n",
    "To obtain these app credentials, refer to the [Apps and tokens guide](https://developer.seequent.com/docs/guides/getting-started/apps-and-tokens) in the Seequent Developer Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evo.notebooks import FeedbackWidget, ServiceManagerWidget\n",
    "from evo.objects import ObjectAPIClient\n",
    "\n",
    "cache_location = \"data\"\n",
    "\n",
    "# Evo app credentials\n",
    "client_id = \"<your-client-id>\"  # Replace with your client ID\n",
    "redirect_url = \"<your-redirect-url>\"  # Replace with your redirect URL\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    discovery_url=\"https://discover.api.seequent.com\",\n",
    "    redirect_url=redirect_url,\n",
    "    client_id=client_id,\n",
    "    cache_location=cache_location,\n",
    ").login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Evo Python SDK to create an object client and a data client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object client will manage your auth token and Geoscience Object API requests.\n",
    "object_client = ObjectAPIClient(manager.get_environment(), manager.get_connector())\n",
    "\n",
    "# The data client will manage saving your data as Parquet and publishing your data to Evo storage.\n",
    "data_client = object_client.get_data_client(manager.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all objects in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "all_objects = await object_client.list_all_objects()\n",
    "\n",
    "table = PrettyTable([\"Name\", \"Object ID\"])\n",
    "for index, obj in enumerate(all_objects):\n",
    "    if \"pointset\" in obj.schema_id.sub_classification:\n",
    "        table.add_row([obj.name.ljust(20), str(obj.id).ljust(40)])\n",
    "\n",
    "if len(table.rows) == 0:\n",
    "    print(\"No pointsets found. Publish a pointset using the 'publish-pointset' notebook.\")\n",
    "else:\n",
    "    print(\"Pointsets found:\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter the `Object ID` value for the chosen pointset object in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = \"<your-object-id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Parquet files and assemble the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_object = await object_client.download_object_by_id(object_id=object_id)\n",
    "\n",
    "metadata = downloaded_object.metadata\n",
    "downloaded_dict = downloaded_object.as_dict()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Use the data client to download the coordinate data.\n",
    "downloaded_data = await data_client.download_table(\n",
    "    object_id=metadata.id,\n",
    "    version_id=metadata.version_id,\n",
    "    table_info=downloaded_dict[\"locations\"][\"coordinates\"],\n",
    "    fb=FeedbackWidget(f\"Downloading coordinates data as '{downloaded_dict['locations']['coordinates']['data']}'\"),\n",
    ")\n",
    "\n",
    "df = pd.concat([df, downloaded_data.to_pandas()], axis=1)\n",
    "\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "for attribute in downloaded_dict[\"locations\"][\"attributes\"]:\n",
    "    attribute_name = attribute[\"name\"]\n",
    "    attribute_type = attribute[\"attribute_type\"]\n",
    "\n",
    "    # Download the attribute data. Every attribute has a 'values' data file.\n",
    "    values_data = await data_client.download_table(\n",
    "        object_id=metadata.id,\n",
    "        version_id=metadata.version_id,\n",
    "        table_info=attribute[\"values\"],\n",
    "        fb=FeedbackWidget(f\"Downloading attribute '{attribute_name}' values data as '{attribute['values']['data']}'\"),\n",
    "    )\n",
    "\n",
    "    # If the attribute is a category, download the 'table' data as well.\n",
    "    if attribute_type == \"category\":\n",
    "        table_data = await data_client.download_table(\n",
    "            object_id=metadata.id,\n",
    "            version_id=metadata.version_id,\n",
    "            table_info=attribute[\"table\"],\n",
    "            fb=FeedbackWidget(f\"Downloading attribute '{attribute_name}' table data as '{attribute['table']['data']}'\"),\n",
    "        )\n",
    "\n",
    "    # Assemble the dataframe.\n",
    "    if attribute_type == \"category\":\n",
    "        # Merge the values data with the table data.\n",
    "        merged_data = pd.merge(\n",
    "            values_data.to_pandas(), table_data.to_pandas(), left_on=\"data\", right_on=\"key\", how=\"left\"\n",
    "        )\n",
    "        # Drop the 'data' and 'key' columns from the merged data.\n",
    "        merged_data.drop(columns=[\"data\", \"key\"], inplace=True)\n",
    "        # Rename the 'value' column to the attribute name.\n",
    "        merged_data.rename(columns={\"value\": attribute_name}, inplace=True)\n",
    "        # Concatenate the merged data with the coordinates data.\n",
    "        df = pd.concat([df, merged_data], axis=1)\n",
    "\n",
    "    elif attribute_type == \"scalar\":\n",
    "        data = values_data.to_pandas()\n",
    "        # Rename the 'data' column to the attribute name.\n",
    "        data.rename(columns={\"data\": attribute_name}, inplace=True)\n",
    "        # Concatenate the data with the coordinates data.\n",
    "        df = pd.concat([df, data], axis=1)\n",
    "\n",
    "# Save the dataframe as a CSV file.\n",
    "df.to_csv(f\"{cache_location}/output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! You now have a new CSV file constructed from an Evo pointset object.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this example, we've completed the following:\n",
    "* Listed all objects in an Evo workspace.\n",
    "* Selected the ID of a pointset object.\n",
    "* Downloaded all Parquet files associated with the pointset.\n",
    "* Constructed a CSV file from the Parquet files and saved the file to disk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
